There are 4 main activation functions:

Relu - Becomes positive at a specific value. Goes from 0 to inf as input increases. Output equal to input, give that the output is greater than 0.

Sigmoid - Gradually goes from 0 to 1 depending on a specific value. Only goes from 0 to 1.

Tanh - Identical to sigmoid besides the fact it goes from -1 to 1, leading to negative correlation possibilities.

Softmax - Reduces outputs to a range of 0 to 1 to help see the probabilities of a given input.